#training.ini Compare to 12
[default]
; random seed for torch
seed = 4
; number of batches parsed through before printing info
print_freq = 5
; save model every save_model_freq'th epoch
save_model_freq = 2
; data for model training (train -> val as validation data, trainval -> test as validation data)
training_dataset = trainval


[hyperparameters]
; number of epochs (epochs defined as 'qsize_class' numbers of imgs from each class)
epochs = 15
; initial kl_div scale for loss function
kl_scale_init = 1e-6
; kl_div scale for loss function at last epoch (exponential growth)
kl_scale_end = 1e-3
; ratio of epochs for warming up kl_div
kl_warmup = 0.25
; determines whether margin is fixed or based on average dist to negatives
margin_fixed = True
; if margin is fixed, this is the absolute distance, otherwise multi with avg.dist
margin_val = 0.0
; variance of prior
var_prior = 0.1
; number of queries in one batch
batch_size = 60
; number of batches parsed throug before doing a gradient update
update_every = 1
; (NOT IN USE) number of epoch before updating backbone repr. pool (also new image augmentation for train)
update_pool_every = 0
; gradient clipping val
clip = 1
; learning_rate at first epoch
lr_init = 1e-3
; learning_rate at last epoch (exponential growth)
lr_end = 1e-5
; probability of skipping closest negative when creating tuples
skip_closest_neg_prob = 0.0

[model]
; backbone architecture
architecture = 'resnet152'
; should backbone model be fixed? (SHOULD BE True, not implemented correctly for False)
fixed_backbone = True
; should backbone model be kept in eval mode?
const_eval_mode = True
; dim of mean and variance fc head
head_layers_dim = {'mean': [100], 'var': [40,20]}
; activation function for non-linearity
activation_fn = {'type': 'relu', 'param': None}
; GeM pooling parameter (power)
pooling = {'mGeM_p': 3,'vGeM_p': 2}
; output dim, if 'iso' -> mean: (n-1), var: (1), if 'diag' -> mean: (n/2), var: (n/2)
dim_out = 20
; dropout perc
dropout = 0.50
; variance type
var_type = 'diag'
; with or without swag estimates
with_swag = True

[image_transform]
; img size
img_size = 224
; augmentations of training images
augmentation = True
; normalization for imgs.
normalize = {'mean': [0.485, 0.456, 0.406],'var' : [0.229, 0.224, 0.225]}

[dataset]
; number of negatives for each query
nnum = 10
; number of queries for each class per triplet batch
qsize_class = 40
; number of images for each class in training pool
poolsize_class = 20000
; number of images in negative pool
npoolsize = 3000
; should we keep tuples from previous generation?
keep_prev_tuples = False
; classes not trained on
classes_not_trained_on = ['horse','car']
; TupleLoader approximative similarity measure
approx_similarity = False
; distribution of classes for negative ('uniform', 'unif_to_free', 'free')
neg_class_dist = 'uniform'