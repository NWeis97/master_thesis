#training.ini
[default]
; random seed for torch
seed = 1234 
; number of batches parsed through before printing info
print_freq = 5
; save model every save_model_freq'th epoch
save_model_freq = 5

[hyperparameters]
; number of epochs (epochs defined as 'qsize_class' numbers of imgs from each class)
epochs = 100
; initial kl_div scale for loss function
kl_scale_init = 1e-8
; kl_div scale for loss function at last epoch (exponential growth)
kl_scale_end = 1e-3
; ratio of epochs for warming up kl_div
kl_warmup = 0.25
; determines whether margin is fixed or based on average dist to negatives
margin_fixed = True
; if margin is fixed, this is the absolute distance, otherwise multi with avg.dist
margin_val = 1
; variance of prior
var_prior = 1
; number of queries in one batch
batch_size = 15
; number of batches parsed throug before doing a gradient update
update_every = 5
; gradient clipping val
clip = 1
; learning_rate at first epoch
lr_init = 3e-3
; learning_rate at last epoch (exponential growth)
lr_end = 5e-5

[model]
; backbone architecture
architecture = 'resnet152'
; should backbone model be fixed?
fixed_backbone = True
; should backbone model be kept in eval mode?
const_eval_mode = True
; dim of mean and variance fc head
head_layers_dim = {'mean': [200,100], 'var': [500,250,100]}
; activation function for non-linearity
activation_fn = {'type': 'relu', 'param': None}
; GeM pooling parameter (power)
pooling = {'mGeM_p': 1,'vGeM_p': 2}
; output dim, mean: (n-1), var: 1 
dim_out = 20
; dropout perc. for fc layers
dropout = 0.40

[image_transform]
; img size
img_size = 224
; augmentations of training images (STILL TO BE IMPLEMENTED)
augmentation = False
; normalization for imgs.
normalize = {'mean': [0.485, 0.456, 0.406],'var' : [0.229, 0.224, 0.225]}

[dataset]
; number of negatives for each query
nnum = 10
; number of queries for each class
qsize_class = 50
; number of negatives in pool for each query
poolsize = 1000
; should we keep tuples from previous generation?
keep_prev_tuples = False